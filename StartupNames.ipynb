{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import newspaper\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "from newspaper import Article\n",
    "from newspaper import Config\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions as EC \n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "# For Spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# For Newspaper3k\n",
    "user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'\n",
    "config = Config()\n",
    "config.browser_user_agent = user_agent\n",
    "\n",
    "# For Selenium\n",
    "option = webdriver.ChromeOptions()\n",
    "option.add_argument(\" — incognito\")\n",
    "executable_path = '/Users/kenmiyachi/Desktop/chromedriver'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_base_url(url):\n",
    "    return '/'.join(url.split('/')[:3])\n",
    "\n",
    "def print_with_newlines(strings):\n",
    "    for s in strings:\n",
    "        print(s + '\\n')\n",
    "\n",
    "def print_summary(article):\n",
    "    article.nlp()\n",
    "    strings = [article.title, article.authors, article.keywords, article.summary]\n",
    "    print_with_newlines(strings)\n",
    "    print('\\n-------------------\\n')\n",
    "    \n",
    "def form_query_strings(search_query):\n",
    "    search_terms = search_query.split(' ')\n",
    "    plus_query = '+'.join(search_terms)\n",
    "    percent_query = '%20'.join(search_terms)\n",
    "    return plus_query, percent_query\n",
    "        \n",
    "def get_articles(url, base_url, max_articles):\n",
    "    \"\"\"Returns all scraped articles from a website up to a maximum number of articles\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        The url of the website including the page number\n",
    "    base_url : str\n",
    "        The root url of the website\n",
    "    max_articles : int\n",
    "        The maximum number of articles that will be scraped \n",
    "        (will be less than this if there are not enough articles on the webpage)\n",
    "    \"\"\"\n",
    "    assert(base_url[-1] != '/')\n",
    "    articles = []\n",
    "    \n",
    "    if base_url == 'https://www.coindesk.com': return get_coindesk_articles(url, base_url, max_articles)\n",
    "    elif base_url == 'https://cointelegraph.com': return get_cointelegraph_articles(url, base_url, max_articles)\n",
    "    elif base_url == 'https://www.forbes.com': return get_forbes_articles(url, base_url, max_articles)\n",
    "    elif base_url == 'https://nulltx.com': return get_nulltx_articles(url, base_url, max_articles)\n",
    "    elif base_url == 'https://techstartups.com': return get_techstartups_articles(url, base_url, max_articles)\n",
    "    elif base_url == 'https://medium.com': return get_medium_articles(url, base_url, max_articles)\n",
    "    elif base_url == 'https://www.newsbtc.com': return get_newsbtc_articles(url, base_url, max_articles)\n",
    "    elif base_url == 'https://bitcoinist.com': return get_bitcoinist_articles(url, base_url, max_articles)\n",
    "    \n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    all_links = soup.findAll('a', href=True)\n",
    "\n",
    "    for link in all_links:\n",
    "        link = link['href']\n",
    "\n",
    "        # Format hrefs that only include the endpoint names\n",
    "        if link != '' and link[0] == '/':\n",
    "            link = base_url + link\n",
    "            \n",
    "        # Check for valid urls\n",
    "        if (base_url in link and '#' not in link and len(link) > len(base_url) + 1 \n",
    "            and '-' in ''.join(link.split('/')[3:])):\n",
    "            try:\n",
    "                print(link)\n",
    "                article = get_article(link)\n",
    "                if article and article.title not in [article.title for article in articles]:\n",
    "                    articles.append(article)\n",
    "                    print('added')\n",
    "                    if len(articles) >= max_articles:\n",
    "                        return articles\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print('%s\\n' % e)\n",
    "\n",
    "    return articles\n",
    "\n",
    "# Helper function for get_articles\n",
    "def get_article(link):\n",
    "    article = Article(link, config=config)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "\n",
    "    if article.publish_date: # Check if link is a valid article\n",
    "        return article\n",
    "    return None\n",
    "\n",
    "def extract_startup_names(article):\n",
    "    startup_names = []\n",
    "    print_with_newlines([article.title])\n",
    "    doc = nlp(article.text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'ORG' and '@' not in ent.text:\n",
    "            #print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "            startup_names.append(ent.text)\n",
    "        \n",
    "    return startup_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Helper Functions (For 3 types of websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_paged_site(url, base_url, max_articles, articles_per_page, get_page_url, scrape_links):\n",
    "    \"\"\"Scrapes website that has multiple pages that are accessible by changing the url\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        The url of the website including the page number\n",
    "    base_url : str\n",
    "        The root url of the website\n",
    "    max_articles : int\n",
    "        The maximum number of articles that will be scraped \n",
    "        (will be less than this if there are not enough articles on the webpage)\n",
    "    articles_per_page : int\n",
    "        The number of articles on a single page\n",
    "    get_page_url : function\n",
    "        Function that take as input a page number and returns a url for the page\n",
    "    scrape_links : function\n",
    "        Function that returns all scraped article urls on the webpage\n",
    "    \"\"\"\n",
    "    assert(base_url[-1] != '/')\n",
    "    articles = []\n",
    "    \n",
    "    for i in range(1, int(max_articles / articles_per_page) + 1):\n",
    "        url = get_page_url(url, base_url, i)\n",
    "        print(url)\n",
    "        try:\n",
    "            page = requests.get(url)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        for link in scrape_links(soup):\n",
    "            try:\n",
    "                print(link)\n",
    "                article = get_article(link)\n",
    "                if article and article.title not in [article.title for article in articles]:\n",
    "                    articles.append(article)\n",
    "                    print('added')\n",
    "                    if len(articles) >= max_articles:\n",
    "                        return articles\n",
    "\n",
    "            except Exception as e:\n",
    "                print('%s\\n' % e)\n",
    "    return articles\n",
    "\n",
    "def scrape_scroll_site(url, max_articles, articles_per_page, scrape_links):\n",
    "    \"\"\"Scrapes website that generates new content as the user scrolls down\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        The url of the website including the page number\n",
    "    max_articles : int\n",
    "        The maximum number of articles that will be scraped \n",
    "        (will be less than this if there are not enough articles on the webpage)\n",
    "    articles_per_page : int\n",
    "        The number of articles on a single page\n",
    "    scrape_links : function\n",
    "        Function that returns all scraped article urls on the webpage\n",
    "    \"\"\"\n",
    "    for attempt in range(10):\n",
    "        try:\n",
    "            browser = webdriver.Chrome(executable_path=executable_path, chrome_options=option)\n",
    "            browser.get(url)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('trying again')\n",
    "            time.sleep(10)\n",
    "            break\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "    for _ in range(int(np.ceil(max_articles / articles_per_page))): # Ensures that at least max_articles count can be scraped\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)\n",
    "    \n",
    "    articles = []\n",
    "    soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "    \n",
    "    for link in scrape_links(soup):\n",
    "        print(link)\n",
    "        try:\n",
    "            print(link)\n",
    "            article = get_article(link)\n",
    "            if article and article.title not in [article.title for article in articles]:\n",
    "                articles.append(article)\n",
    "                print('added')\n",
    "                if len(articles) >= max_articles:\n",
    "                    return articles\n",
    "\n",
    "        except Exception as e:\n",
    "            print('%s\\n' % e)\n",
    "\n",
    "    return articles\n",
    "\n",
    "def scrape_button_site(url, max_articles, articles_per_page, seemore_xpath, scrape_links, close_toast_xpath=''):\n",
    "    \"\"\"Scrapes website that has a button user presses to show more articles on the page\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        The url of the website including the page number\n",
    "    max_articles : int\n",
    "        The maximum number of articles that will be scraped \n",
    "        (will be less than this if there are not enough articles on the webpage)\n",
    "    articles_per_page : int\n",
    "        The number of articles on a single page\n",
    "    seemore_xpath : str or function\n",
    "        The string or string returning function that is the xpath of the button that generates more articles\n",
    "    scrape_links : function\n",
    "        Function that returns all scraped article urls on the webpage\n",
    "    close_toast_xpath: str, optional\n",
    "        The xpath of the button that closes toast pop-ups that inform the user about cookies\n",
    "    \"\"\"\n",
    "    for attempt in range(10):\n",
    "        try:\n",
    "            browser = webdriver.Chrome(executable_path=executable_path, chrome_options=option)\n",
    "            browser.get(url)\n",
    "\n",
    "            # Close the \"accept cookies\" toast pop-up\n",
    "            if close_toast_xpath != '':\n",
    "                WebDriverWait(browser, 20).until(EC.element_to_be_clickable((By.XPATH, close_toast_xpath)))\n",
    "                WebDriverWait(browser, 20).until(EC.visibility_of_element_located((By.XPATH, close_toast_xpath)))\n",
    "                time.sleep(3)\n",
    "                browser.find_element_by_xpath(close_toast_xpath).click()\n",
    "                time.sleep(3)\n",
    "\n",
    "            # Click on \"See More\" button\n",
    "            for i in range(1, int(np.ceil(max_articles / articles_per_page))+1):\n",
    "                try:\n",
    "                    if callable(seemore_xpath):\n",
    "                        seemore_xpath_string = seemore_xpath(i)\n",
    "                    else:\n",
    "                        seemore_xpath_string = seemore_xpath\n",
    "                    WebDriverWait(browser, 20).until(EC.visibility_of_element_located((By.XPATH, seemore_xpath_string)))\n",
    "                    python_button = browser.find_element_by_xpath(seemore_xpath_string)\n",
    "                    python_button.click()\n",
    "                    time.sleep(3)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('trying again')\n",
    "            time.sleep(10)\n",
    "            break\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "    articles = []\n",
    "    soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "    for link in scrape_links(soup):\n",
    "        try:\n",
    "            print(link)\n",
    "            article = get_article(link)\n",
    "            if article and article.title not in [article.title for article in articles]:\n",
    "                articles.append(article)\n",
    "                print('added')\n",
    "                if len(articles) >= max_articles:\n",
    "                    return articles\n",
    "        except Exception as e:\n",
    "            print('%s\\n' % e)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Functions for Specific Websites (All wrapped in the Get_Articles function above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_url(url, base_url, page_num):\n",
    "    return base_url + '/page/' + str(i) + '/' + url.split('/')[-1]\n",
    "\n",
    "def get_nulltx_articles(url, base_url, max_articles):\n",
    "    def scrape_links(soup):\n",
    "        links = []\n",
    "        all_items = soup.findAll('a', rel='bookmark', href=True)\n",
    "        for link in all_items:\n",
    "            links.append(link['href'])\n",
    "        return links\n",
    "    \n",
    "    articles_per_page = 19\n",
    "    #page_urls = get_page_urls(url, base_url, max_articles, articles_per_page)\n",
    "    articles = scrape_paged_site(url, base_url, max_articles, articles_per_page, get_page_url, scrape_links)\n",
    "    return articles\n",
    "\n",
    "def get_medium_articles(url, base_url, max_articles):\n",
    "    def scrape_links(soup):\n",
    "        links = []\n",
    "        all_items = soup.findAll('div', class_='postArticle-content')\n",
    "        for link in all_items:\n",
    "            links.append(link.find('a', href=True)['href'])\n",
    "        return links\n",
    "    \n",
    "    articles_per_page = 10\n",
    "    articles = scrape_scroll_site(url, max_articles, articles_per_page, scrape_links)\n",
    "    return articles\n",
    "\n",
    "def get_techstartups_articles(url, base_url, max_articles):\n",
    "    def scrape_links(soup):\n",
    "        links = []\n",
    "        all_items = soup.findAll('div', class_='post_img static one_third')\n",
    "        for link in all_items:\n",
    "            links.append(link.find('a', href=True)['href'])\n",
    "        return links\n",
    "    \n",
    "    articles_per_page = 15\n",
    "    #page_urls = get_page_urls(url, base_url, max_articles, articles_per_page)\n",
    "    articles = scrape_paged_site(url, base_url, max_articles, articles_per_page, get_page_url, scrape_links)\n",
    "    return articles\n",
    "\n",
    "def get_coindesk_articles(url, base_url, max_articles):\n",
    "    def scrape_links(soup):\n",
    "        links = []\n",
    "        all_items = soup.findAll('div', class_='text-content')\n",
    "        for link in all_items:\n",
    "            endpoint = link.findAll('a')[1]['href']\n",
    "            link = 'https://www.coindesk.com' + str(endpoint)\n",
    "            links.append(link)\n",
    "        return links\n",
    "    \n",
    "    def get_seemore_xpath(i):\n",
    "        xpath = '//*[@id=\"__next\"]/main/section/div/section[2]/article/div/section/div[%d1]/button' % i\n",
    "        return xpath\n",
    "    \n",
    "    articles_per_page = 10\n",
    "    articles = scrape_button_site(url, max_articles, articles_per_page, get_seemore_xpath, scrape_links)\n",
    "    return articles\n",
    "\n",
    "def get_cointelegraph_articles(url, base_url, max_articles):\n",
    "    def scrape_links(soup):\n",
    "        links = []\n",
    "        all_items = soup.findAll('h2', class_='header')\n",
    "        for link in all_items:\n",
    "            links.append(link.find('a')['href'])\n",
    "        return links\n",
    "    \n",
    "    seemore_xpath = '//*[@id=\"search-results\"]/div/div[3]/nav/div/div[2]/a'\n",
    "    close_toast_xpath = '//*[@id=\"vue-footer\"]/div/div/div/div[2]/a[2]'\n",
    "    articles_per_page = 28\n",
    "    articles = scrape_button_site(url, max_articles, articles_per_page, seemore_xpath, scrape_links, close_toast_xpath)\n",
    "    return articles\n",
    "\n",
    "def get_forbes_articles(url, base_url, max_articles):\n",
    "    def scrape_links(soup):\n",
    "        links = []\n",
    "        all_items = soup.findAll('a', class_='stream-item__title', href=True)\n",
    "        for link in all_items:\n",
    "            links.append(link['href'])\n",
    "        return links\n",
    "    \n",
    "    seemore_xpath = '/html/body/div[1]/main/div[1]/div[1]/div[5]'\n",
    "    close_toast_xpath = '//*[@id=\"truste-consent-button\"]'\n",
    "    articles_per_page = 20\n",
    "    articles = scrape_button_site(url, max_articles, articles_per_page, seemore_xpath, scrape_links, close_toast_xpath)\n",
    "    return articles\n",
    "\n",
    "def get_newsbtc_articles(url, base_url, max_articles):\n",
    "    def scrape_links(soup):\n",
    "        links = []\n",
    "        all_items = soup.findAll('h2', class_='title medium')\n",
    "        for link in all_items:\n",
    "            links.append(link.find('a', href=True)['href'])\n",
    "        return links\n",
    "\n",
    "    seemore_xpath = '//*[@id=\"content\"]/div[2]/div/div/div[2]/span'\n",
    "    close_toast_xpath = '/html/body/div[2]/div/div[2]/div[1]/a[2]'\n",
    "    articles_per_page = 3*8\n",
    "    articles = scrape_button_site(url, max_articles, articles_per_page, seemore_xpath, scrape_links, close_toast_xpath)\n",
    "    return articles\n",
    "\n",
    "def get_bitcoinist_articles(url, base_url, max_articles):\n",
    "    def scrape_links(soup):\n",
    "        links = []\n",
    "        all_items = soup.findAll('h3', class_='title')\n",
    "        for link in all_items:\n",
    "            link = link.find('a', href=True)\n",
    "            if link:\n",
    "                links.append(link['href'])\n",
    "        return links\n",
    "    \n",
    "    seemore_xpath = '//*[@id=\"content\"]/div[2]/section/div[2]/a'\n",
    "    articles_per_page = 3*3\n",
    "    articles = scrape_button_site(url, max_articles, articles_per_page, seemore_xpath, scrape_links)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of websites to choose from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "not_working = [\n",
    "    # Possible to write code for these with a bit of time\n",
    "    'https://www.wired.com/search/?q=%s&page=1&sort=score' % percent_query,\n",
    "    'https://bitcoinmagazine.com/search?text=%s&page=1' % percent_query,\n",
    "    'https://www.tomshardware.com/search?searchTerm=%s' % plus_query, \n",
    "    'https://www.cnet.com/search/?query=%s' % plus_query,\n",
    "    'https://www.ccn.com/?s=%s' % plus_query,\n",
    "    'https://gigaom.com/?s=%s' % plus_query,\n",
    "    \n",
    "    # False positives; dates found for irrelevant articles - May need specific code for scraping\n",
    "    'https://mashable.com/search/?t=stories&q=%s' % percent_query,\n",
    "    'https://thenextweb.com/?q=%s' % percent_query,\n",
    "    'https://www.firstpost.com/search?q=%s' % percent_query,\n",
    "    'https://www.theverge.com/search?q=%s' % plus_query,\n",
    "    \n",
    "    # No search feature\n",
    "    'https://www.todayonchain.com/',\n",
    "    \n",
    "    # Results may not be that helpful\n",
    "    'https://cryptoslate.com/?s=%s' % plus_query,\n",
    "    'https://search.techcrunch.com/search?p=%s&fr2=sb-top&fr=techcrunch' % percent_query,\n",
    "    \n",
    "]\n",
    "\n",
    "working = [\n",
    "    'https://www.coindesk.com/search?q=%s&s=relevant' % percent_query,\n",
    "    'https://cointelegraph.com/search?query=%s' % percent_query,\n",
    "    'https://nulltx.com/?s=%s' % plus_query,\n",
    "    'https://techstartups.com//?s=%s' % plus_query,\n",
    "    'https://www.forbes.com/search/?q=%s' % plus_query,\n",
    "    'https://medium.com/search?q=%s' % percent_query,\n",
    "    'https://www.newsbtc.com/?s=%s&lang=en' % plus_query,\n",
    "    'https://bitcoinist.com/?s=%s&lang=en' % plus_query,\n",
    "]\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://cointelegraph.com/search?query=top%20blockchain\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kenmiyachi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:120: DeprecationWarning: use options instead of chrome_options\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "\n",
      "https://cointelegraph.com/news/cointelegraph-announces-chinese-hq-bolstering-its-international-expansion\n",
      "added\n",
      "https://cointelegraph.com/news/cross-border-blockchain-b2b-volume-to-hit-44-trillion-by-2024\n",
      "added\n",
      "https://cointelegraph.com/news/bitfinex-hack-new-twist-two-arrested-in-israel-after-15m-moved\n",
      "added\n",
      "https://cointelegraph.com/news/major-blockchain-investors-arrington-xrp-neo-global-back-dex-focused-startup\n",
      "added\n",
      "\n",
      "https://www.coindesk.com/search?q=top%20blockchain&s=relevant\n",
      "\n",
      "https://www.coindesk.com/ibm-blockchain-vp-every-dollar-spent-on-blockchain-yields-15-on-cloud\n",
      "added\n",
      "https://www.coindesk.com/discussion-us-crypto-taxes-are-a-nightmare-could-these-proposals-help\n",
      "added\n",
      "https://www.coindesk.com/why-defis-billion-dollar-milestone-matters\n",
      "added\n",
      "https://www.coindesk.com/now-more-than-ever-serc-is-scrutinizing-unregistered-token-offerings\n",
      "added\n",
      "https://www.coindesk.com/chinas-coronavirus-whistleblower-is-now-memorialized-on-ethereum\n",
      "added\n",
      "https://www.coindesk.com/95-confidence-ethereum-developers-pencil-in-july-2020-for-eth-2-0-launch\n",
      "added\n",
      "https://www.coindesk.com/how-blockchain-will-track-taxes-and-tax-cheats\n",
      "added\n",
      "https://www.coindesk.com/sec-commissioner-hester-peirce-proposes-3-year-safe-harbor-period-for-crypto-token-sales\n",
      "added\n",
      "https://www.coindesk.com/blockstack-nodes-will-be-paid-in-btc-not-stx-to-secure-the-network\n",
      "added\n",
      "https://www.coindesk.com/how-the-long-tail-of-the-coronavirus-might-slow-bitcoins-hash-power-growth\n",
      "added\n",
      "https://www.coindesk.com/libra-head-not-worried-about-the-leavers\n",
      "added\n",
      "https://www.coindesk.com/what-is-going-on-with-jae-kwon-and-cosmos\n",
      "added\n",
      "https://www.coindesk.com/handshakes-uncensorable-web-domains-go-live-on-mainnet\n",
      "added\n",
      "https://www.coindesk.com/southern-indian-state-to-launch-dedicated-blockchain-incubator\n",
      "added\n",
      "https://www.coindesk.com/prof-michael-sung-talks-about-the-rise-of-the-digital-yuan\n",
      "added\n",
      "https://www.coindesk.com/pooltogether-defi-app-announces-1m-investment-after-no-loss-lottery-payout-tops-1k\n",
      "added\n",
      "https://www.coindesk.com/mit-develops-spider-tech-to-enable-more-efficient-off-chain-crypto-transactions\n",
      "added\n",
      "https://www.coindesk.com/zcashs-funding-vote-and-the-problems-with-decentralized-governance\n",
      "added\n",
      "https://www.coindesk.com/scotlands-bitcoiners-share-canny-tales-of-buying-the-dip\n",
      "added\n",
      "https://www.coindesk.com/few-banks-will-touch-crypto-firms-but-silvergate-wants-to-touch-bitcoin-itself\n",
      "added\n",
      "\n",
      "https://nulltx.com/?s=top+blockchain\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_page_urls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-dc2937024972>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchosen_sites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchosen_sites\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mall_articles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchosen_sites\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_base_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchosen_sites\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_articles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-d51261d88a24>\u001b[0m in \u001b[0;36mget_articles\u001b[0;34m(url, base_url, max_articles)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mbase_url\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'https://cointelegraph.com'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mget_cointelegraph_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_articles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mbase_url\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'https://www.forbes.com'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mget_forbes_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_articles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mbase_url\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'https://nulltx.com'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mget_nulltx_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_articles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mbase_url\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'https://techstartups.com'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mget_techstartups_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_articles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mbase_url\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'https://medium.com'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mget_medium_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_articles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-e917798c233c>\u001b[0m in \u001b[0;36mget_nulltx_articles\u001b[0;34m(url, base_url, max_articles)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0marticles_per_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m19\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mpage_urls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_page_urls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_articles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marticles_per_page\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0marticles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_paged_site\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_articles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marticles_per_page\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_page_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscrape_links\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marticles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_page_urls' is not defined"
     ]
    }
   ],
   "source": [
    "# Enter search query here\n",
    "search_query = 'top blockchain'\n",
    "\n",
    "# Enter max number of articles to retrieve from each site\n",
    "max_articles = 20\n",
    "\n",
    "\n",
    "plus_query, percent_query = form_query_strings(search_query)\n",
    "\n",
    "# Can be the same as \"working\"\n",
    "chosen_sites = [\n",
    "    'https://cointelegraph.com/search?query=%s' % percent_query,\n",
    "    'https://www.coindesk.com/search?q=%s&s=relevant' % percent_query,\n",
    "    'https://nulltx.com/?s=%s' % plus_query,\n",
    "    'https://techstartups.com//?s=%s' % plus_query,\n",
    "    'https://medium.com/search?q=%s' % percent_query,\n",
    "    'https://www.newsbtc.com/?s=%s&lang=en' % plus_query,\n",
    "    'https://bitcoinist.com/?s=%s&lang=en' % plus_query,\n",
    "]\n",
    "\n",
    "\n",
    "all_articles = defaultdict() # key is index, value is articles\n",
    "try:\n",
    "    for i in range(len(chosen_sites)):\n",
    "        print(chosen_sites[i] + '\\n')\n",
    "        all_articles[i] = get_articles(chosen_sites[i], get_base_url(chosen_sites[i]), max_articles)\n",
    "        print()\n",
    "\n",
    "except KeyboardInterrupt: # Save articles in case of error\n",
    "    #pickle.dump(all_articles, open('all_articles_backup.p', 'wb'))\n",
    "    pass\n",
    "#except Exception as e:\n",
    "    #pickle.dump(all_art icles, open('all_articles_backup.p', 'wb'))\n",
    "    #print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(all_articles)):\n",
    "    print(len(all_articles[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Save articles\n",
    "#pickle.dump(all_articles, open('all_articles.p', 'wb'))\n",
    "\n",
    "# Test that it is saved properly\n",
    "#saved_articles = pickle.load(open('all_articles.p', 'rb'))\n",
    "#saved_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Startup Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Cointelegraph Announces Chinese HQ, Bolstering Its International Expansion\n",
      "\n",
      "Global Blockchain B2B Volume Expected to Hit $4.4 Trillion by 2024\n",
      "\n",
      "Bitfinex Hack New Twist: Two Arrested in Israel After $1.5M Moved\n",
      "\n",
      "Major Blockchain Investors Arrington XRP, NEO Global Back DEX-Focused Startup\n",
      "\n",
      "1\n",
      "IBM Blockchain VP: Every Dollar Spent on Blockchain Yields $15 on Cloud\n",
      "\n",
      "US Crypto Taxes Are a Nightmare. Could These Proposals Help?\n",
      "\n",
      "Why DeFi’s Billion-Dollar Milestone Matters\n",
      "\n",
      "Now More Than Ever, SEC Is Scrutinizing Unregistered Token Offerings\n",
      "\n",
      "China’s Coronavirus Whistleblower Is Now Memorialized on Ethereum\n",
      "\n",
      "‘95% Confidence’: Ethereum Developers Pencil In July 2020 for Eth 2.0 Launch\n",
      "\n",
      "How Blockchain Will Track Taxes (and Tax Cheats)\n",
      "\n",
      "SEC Commissioner Hester Peirce Proposes 3-Year Safe Harbor Period for Crypto Token Sales\n",
      "\n",
      "Blockstack’s New Consensus Mechanism Creates New Use Case for Bitcoin\n",
      "\n",
      "How the Long Tail of the Coronavirus Might Slow Bitcoin’s Hash Power Growth\n",
      "\n",
      "Libra Vice Chair Not Worried About the Leavers\n",
      "\n",
      "What Is Going On With Jae Kwon and Cosmos?\n",
      "\n",
      "Handshake’s Uncensorable Web Domains Go Live on Mainnet\n",
      "\n",
      "Southern Indian State to Launch Dedicated Blockchain Incubator\n",
      "\n",
      "What Are China’s Goals Behind Its Digital Yuan?\n",
      "\n",
      "PoolTogether DeFi App Announces $1M Investment After No-Loss Lottery Payout Tops $1K\n",
      "\n",
      "MIT Develops ‘Spider’ Tech to Enable More Efficient Off-Chain Crypto Transactions\n",
      "\n",
      "Zcash’s Funding Vote and the Woes of Decentralized Governance\n",
      "\n",
      "Scotland’s Bitcoiners Share Canny Tales of ‘Buying the Dip’\n",
      "\n",
      "Few Banks Will Touch Crypto Firms, but Silvergate Wants to Touch Bitcoin Itself\n",
      "\n"
     ]
    }
   ],
   "source": [
    "startup_names = []\n",
    "for key, articles in all_articles.items():\n",
    "    print(key)\n",
    "    for article in articles:\n",
    "        startup_names.extend(extract_startup_names(article))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Extracted Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\"tendermint inc.', 'amentum investment management', 'antminer s9',\n",
       "       'ap', 'api', 'apple podcasts', 'apps', 'arrington',\n",
       "       'artificial intelligence laboratory (csail', 'at&t', 'atom',\n",
       "       'bain capital ventures', 'bank of china', 'bfx', 'big blue',\n",
       "       'bitmain', 'bits inc.', 'block', 'blockstack', 'brave.com',\n",
       "       'buterin', 'canaan creative', 'carnegie mellon university',\n",
       "       'chain capital', 'chinese communist party', 'clarity',\n",
       "       'clipboard hijackers’', 'coindesk', 'coinfund', 'computer science',\n",
       "       'consensys', 'consensys labs', 'cornell university', 'cpc',\n",
       "       'crown', 'csail', 'cuomo', 'cusack', 'dai', 'davispolk', 'defi',\n",
       "       'democratic caucus', 'digital currency research institute',\n",
       "       'disgorgement', 'disparte', 'dtc capital', 'ecc', 'elliptic',\n",
       "       'eos', 'erc-20 eos', 'ernst & young', 'eth', 'eth 1.x', 'eu',\n",
       "       'exchange commission', 'exchanges.\"satoshi', 'facebook',\n",
       "       'federal reserve', 'fenwick & west', 'finastra', 'founder', 'fud',\n",
       "       'fudan university', 'github', 'gladius', 'google podcasts',\n",
       "       'goren holm ventures', 'guardtime', 'hackers', 'hns', 'hosho',\n",
       "       'how roquerre', 'huobi', 'hyperledger fabric', 'ibc', 'ibisworld',\n",
       "       'ibm', 'ibm blockchain world wire', 'ic3 emin gun sirer', 'ico',\n",
       "       'ideo colab ventures', 'infosys finacle', 'innosilicon', 'iphone',\n",
       "       'irs', 'jpmorgan', 'juniper', 'juniper research', 'kotsiuba',\n",
       "       'kwh', 'kwon', 'lane', 'le agencies', 'leighton cusack', 'libra',\n",
       "       'linux', 'mastercard', 'mckie', 'mckinsey & company', 'medium',\n",
       "       'metamask', 'miners', 'mit', 'morgan creek digital', 'mycoin',\n",
       "       'namebase', 'ncg', 'nebulous', 'neo global capital (ngc',\n",
       "       'netizens', 'next', 'nova club', 'nu4', 'okex', 'openshift',\n",
       "       'pandaminer', 'parity technologies', 'pcn', 'peirce',\n",
       "       'pooltogether', 'prized linked savings', 'proof of work (',\n",
       "       'proof-of-stake', 'r3', 'rama devi', 'red hat', 'reuters',\n",
       "       'reward', 'ripplenet', 'rrt', 'rtt', 'sawhney', 'sec', 'sen',\n",
       "       'sen leverage', 'sianotes', 'silvergate', 'silvergate bank',\n",
       "       'silvergate exchange network', 'sivaraman', 'smith', 'stacks',\n",
       "       'supreme court', 'swihart', 'talk bitcoin', 'techcrunch',\n",
       "       'telangana', 'telegram', 'tendermint', 'tendermint labs',\n",
       "       'the association', 'the cbe foundation',\n",
       "       'the chinese communist party', 'the davos promenade',\n",
       "       'the electric coin company',\n",
       "       'the global blockchain business council',\n",
       "       'the hong kong bitcoin association',\n",
       "       'the indian school of business',\n",
       "       'the international blockchain congress',\n",
       "       'the israel defence forces (idf', 'the it&c department', 'the let',\n",
       "       'the libra association',\n",
       "       'the massachusetts institute of technology', 'the securities',\n",
       "       'the state of cryptocurrency-stealing malware',\n",
       "       'the times of india', 'the u.s. securities',\n",
       "       'the university of illinois at urbana-champaign',\n",
       "       'the web3 foundation', 'the world economic forum',\n",
       "       'the zcash community forum', 'the zcash foundation', 'thunberg',\n",
       "       'time', 'times', 'top level domain', 'tvl', 'u.s. supreme court',\n",
       "       'usdc', 'users', 'users’', 'utc', 'vapt', 'virtual currency',\n",
       "       'visa', 'visa b2b connect', 'vodafone', 'wang', 'wechat', 'weibo',\n",
       "       'whatsminer', 'wilcox', 'world economic forum',\n",
       "       'wuhan central hospital', 'zcash foundation', 'zec', 'zip',\n",
       "       'zokyo labs', '” zcash foundation executive'], dtype='<U46')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_remove = []\n",
    "\n",
    "unique_startup_names = np.unique(startup_names).tolist()\n",
    "for i, name in enumerate(unique_startup_names):\n",
    "    unique_startup_names[i] = name.strip().lower()\n",
    "    if name == '' or 'http' in name or not re.search('[a-zA-Z]', name):\n",
    "        to_remove.append(i)\n",
    "    else:\n",
    "        name = name.replace(u'’', u\"'\").lower()\n",
    "        for symbol in [',', '\\n', '\\'s', ' and ', '/', ' vs ']:\n",
    "            if symbol in name:\n",
    "                #print(name)\n",
    "                to_remove.append(i)\n",
    "                unique_startup_names.extend(name.split(symbol))\n",
    "\n",
    "to_remove.reverse()\n",
    "for i in to_remove:\n",
    "    unique_startup_names.pop(i)\n",
    "\n",
    "            \n",
    "unique_startup_names = np.unique(np.array(unique_startup_names))\n",
    "unique_startup_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names from the Master List\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Abra',\n",
       " 'Aeternity',\n",
       " 'AlphaPoint',\n",
       " 'AirFox',\n",
       " 'Ardor',\n",
       " 'Ark',\n",
       " 'Ascribe',\n",
       " 'Augur',\n",
       " 'Auroracoin',\n",
       " 'Backfeed',\n",
       " 'BigchainDB',\n",
       " 'Bitfury',\n",
       " 'BitGive',\n",
       " 'Bitmark',\n",
       " 'BitPagos',\n",
       " 'Bitpay',\n",
       " 'BitSE',\n",
       " 'Bitswift',\n",
       " 'BlackCoin',\n",
       " 'Blakecoin',\n",
       " 'BlockMedX',\n",
       " 'Blockphase',\n",
       " 'Blockstream',\n",
       " 'BlockVerify',\n",
       " 'Bloq',\n",
       " 'BTL Group',\n",
       " 'CareX',\n",
       " 'Cashaa',\n",
       " 'Chain Inc.',\n",
       " 'Chain of Things',\n",
       " 'Chainy',\n",
       " 'Circle',\n",
       " 'Cognate',\n",
       " 'Coinbase',\n",
       " 'Colony',\n",
       " 'Colu',\n",
       " 'COMIT',\n",
       " 'ConnectJob',\n",
       " 'Consensys Systems',\n",
       " 'ContentKid',\n",
       " 'CrowdWiz',\n",
       " 'Crowdz',\n",
       " 'CureCoin',\n",
       " 'Cypherium',\n",
       " 'Decent',\n",
       " 'Decissio',\n",
       " 'DFINITY',\n",
       " 'Digital Asset Holdings',\n",
       " 'DMarket',\n",
       " 'Earthport',\n",
       " 'Enigma',\n",
       " 'Epiphyte',\n",
       " 'Ethereum',\n",
       " 'Experty',\n",
       " 'ExtraCredit',\n",
       " 'Filecoin',\n",
       " 'Flowchain',\n",
       " 'Global Blockchain',\n",
       " 'Hello Block',\n",
       " 'Herosphere',\n",
       " 'HIVE BLOCKCHAIN',\n",
       " 'Horizon State',\n",
       " 'Humaniq',\n",
       " 'Hyperledger',\n",
       " 'ICOBox',\n",
       " 'Kalpa Digital Health',\n",
       " 'Komodo',\n",
       " 'KYC-CHAIN',\n",
       " 'LAToken',\n",
       " 'Lisk',\n",
       " 'Loci',\n",
       " 'Luna',\n",
       " 'MazaCoin',\n",
       " 'MintHealth',\n",
       " 'Monax',\n",
       " 'MyriadCoin',\n",
       " 'Odem',\n",
       " 'OmiseGO',\n",
       " 'OpenBazaar',\n",
       " 'Openchain',\n",
       " 'Parity Technologies',\n",
       " 'Peercoin',\n",
       " 'Plex',\n",
       " 'Power Ledger',\n",
       " 'Primecoin',\n",
       " 'Propy',\n",
       " 'Protocol Labs',\n",
       " 'Provenance',\n",
       " 'Qvolta',\n",
       " 'RIOT',\n",
       " 'Ripio',\n",
       " 'Ripple',\n",
       " 'Sensay',\n",
       " 'ShipChain',\n",
       " 'ShoCard',\n",
       " 'Simplex',\n",
       " 'SkyHive',\n",
       " 'SlockIt',\n",
       " 'SmartLedger',\n",
       " 'Synereo',\n",
       " 'Tagcoin',\n",
       " 'TenX',\n",
       " 'Thrive',\n",
       " 't-zero',\n",
       " 'TrustToken',\n",
       " 'Vault One',\n",
       " 'VeChain',\n",
       " 'Veem',\n",
       " 'Warranteer',\n",
       " 'Wave',\n",
       " 'Xapo']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blockchain_startup_names = None\n",
    "with open('blockchain_startup_names.txt', 'r') as f:\n",
    "    blockchain_startup_names = [line.strip() for line in f]\n",
    "print('Names from the Master List')\n",
    "blockchain_startup_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parity technologies\n",
      "\n",
      "Percent of Startups found from the master list: 0.009\n"
     ]
    }
   ],
   "source": [
    "target_startup_names = blockchain_startup_names\n",
    "found = 0\n",
    "for target in target_startup_names:\n",
    "    if target.lower() in unique_startup_names:\n",
    "        print(target.lower())\n",
    "        found += 1\n",
    "print('\\nPercent of Startups found from the master list: %.3f' % (found / len(target_startup_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_choice = all_articles[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cointelegraph Announces Chinese HQ, Bolstering Its International Expansion\n",
      "2019-12-04 19:26:00+00:00\n",
      "To support our international expansion and global reach, Cointelegraph is delighted to announce the launch of the Chinese-language version of the publication. Today, Dec. 4, we celebrated the opening of the office of Cointelegraph China (Cointelegraph 中文).\n",
      "\n",
      "The news — which marks another milestone moment in Cointelegraph’s growth — was announced at the Nova Global Blockchain Investment Institutions Summit hosted by the investment ecosystem alliance, Nova Club. Nova Club was formed by top blockchain organizations and aims to facilitate blockchain project development by consolidating resources and expertise.\n",
      "\n",
      "The new expansion will be led from the heart of Guangzhou, with other offices in Beijing and Shanghai.\n",
      "\n",
      "Meet the Cointelegraph China business team\n",
      "\n",
      "Cointelegraph China has brought together leading names in the industry to highlight blockchain and crypto trends in the area.\n",
      "\n",
      "Co-founder Vadim Krekotin will manage Cointelegraph China’s initial launch. He previously founded advisory blockchain firm the CBE Foundation. Vadim is fluent in Mandarin and has a long history of conducting business in China, which has brought him in contact with the country’s biggest industry players including Binance, Huobi, OkEX and many others.\n",
      "\n",
      "Simon Li is another co-founder of Cointelegraph China and a founding partner of Chain Capital and Nova Club. Li focuses on mining, investment into blockchain projects, and initiating and managing blockchain investment funds.\n",
      "\n",
      "Kevin Shao is a founding investor at Cointelegraph China as well as a general manager at crypto mining equipment manufacturer Canaan-Blockchain. Shao’s professional background includes serving at Bank of China’s fintech and technology department.\n",
      "\n",
      "Stay tuned for editorial team\n",
      "\n",
      "We will soon be announcing the Cointelegraph China editorial team. Our editorial team will produce the highest quality journalism for our readers in China, holding steadfast to the values of editorial independence and responsibility to our readers.\n",
      "\n",
      "Cointelegraph China is now our third base in Asia, following the establishment of Cointelegraph Japan in Tokyo in December 2017 and Cointelegraph Korea in August of this year. China has proved itself a hub for blockchain development, with the support of President Xi Jinping and a slew of blockchain-related patents filed with local regulators.\n",
      "\n",
      "The country is maintaining a hardline stance toward crypto, as cryptocurrency trading is wholly banned in China. Meanwhile, the country has declared plans to issue its own digital currency to compete with the United States dollar in the global market. Our China-based team will work consistently to raise awareness in the region and deliver readers a clear insight into significant industry developments.\n"
     ]
    }
   ],
   "source": [
    "print(article_choice.title)\n",
    "print(article_choice.publish_date)\n",
    "print(article_choice.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loans',\n",
       " 'bitcoin',\n",
       " 'monitor',\n",
       " 'loan',\n",
       " 'banks',\n",
       " 'bank',\n",
       " 'able',\n",
       " 'silvergate',\n",
       " 'crypto',\n",
       " 'touch',\n",
       " 'lane',\n",
       " 'wants',\n",
       " 'sen',\n",
       " 'firms']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article.nlp()\n",
    "article.keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
